#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ÿ™ÿ≥ÿ™ ⁄©ÿßŸÖŸÑ GPU ÿ®ÿ±ÿß€å ÿ≥€åÿ≥ÿ™ŸÖ ÿ™ÿ¥ÿÆ€åÿµ ÿßÿ≠ÿ≥ÿßÿ≥ÿßÿ™
"""

import sys
import time
import numpy as np

def test_pytorch_cuda():
    """ÿ™ÿ≥ÿ™ PyTorch CUDA"""
    print("üîß Testing PyTorch CUDA...")
    try:
        import torch
        print(f"   PyTorch version: {torch.__version__}")
        print(f"   CUDA available: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"   CUDA device count: {torch.cuda.device_count()}")
            print(f"   Current device: {torch.cuda.current_device()}")
            print(f"   Device name: {torch.cuda.get_device_name()}")
            print(f"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
            
            # Test tensor operations on GPU
            start_time = time.time()
            x = torch.randn(1000, 1000).cuda()
            y = torch.randn(1000, 1000).cuda()
            z = torch.matmul(x, y)
            end_time = time.time()
            print(f"   GPU matrix multiplication test: {end_time - start_time:.4f}s")
            return True
        else:
            print("   ‚ùå CUDA not available")
            return False
    except ImportError as e:
        print(f"   ‚ùå PyTorch import error: {e}")
        return False

def test_onnxruntime_gpu():
    """ÿ™ÿ≥ÿ™ ONNX Runtime GPU"""
    print("\nüîß Testing ONNX Runtime GPU...")
    try:
        import onnxruntime as ort
        print(f"   ONNX Runtime version: {ort.__version__}")
        
        providers = ort.get_available_providers()
        print(f"   Available providers: {providers}")
        
        if 'CUDAExecutionProvider' in providers:
            print("   ‚úÖ CUDAExecutionProvider available")
            
            # Test session creation with GPU
            try:
                # Create a dummy session to test GPU
                session_options = ort.SessionOptions()
                session_options.log_severity_level = 3  # Only errors
                # This will fail gracefully if no model exists
                print("   GPU provider test: READY")
                return True
            except Exception as e:
                print(f"   ‚ö†Ô∏è GPU session test failed: {e}")
                return False
        else:
            print("   ‚ùå CUDAExecutionProvider NOT available")
            return False
    except ImportError as e:
        print(f"   ‚ùå ONNX Runtime import error: {e}")
        return False

def test_cupy_optional():
    """ÿ™ÿ≥ÿ™ CuPy (ÿßÿÆÿ™€åÿßÿ±€å)"""
    print("\nüîß Testing CuPy (Optional)...")
    try:
        import cupy as cp
        print(f"   CuPy version: {cp.__version__}")
        
        # Test basic CuPy operation
        start_time = time.time()
        x = cp.random.randn(1000, 1000)
        y = cp.random.randn(1000, 1000)
        z = cp.dot(x, y)
        end_time = time.time()
        print(f"   CuPy matrix multiplication test: {end_time - start_time:.4f}s")
        print("   ‚úÖ CuPy working correctly")
        return True
    except ImportError:
        print("   ‚ÑπÔ∏è CuPy not installed (optional for maximum performance)")
        return False
    except Exception as e:
        print(f"   ‚ùå CuPy error: {e}")
        return False

def get_system_info():
    """ÿßÿ∑ŸÑÿßÿπÿßÿ™ ÿ≥€åÿ≥ÿ™ŸÖ"""
    print("\nüíª System Information:")
    print(f"   Python version: {sys.version}")
    print(f"   Platform: {sys.platform}")
    
def main():
    print("=" * 50)
    print("üöÄ GPU Status Check for Emotion Recognition System")
    print("=" * 50)
    
    get_system_info()
    
    pytorch_ok = test_pytorch_cuda()
    onnx_ok = test_onnxruntime_gpu()
    cupy_ok = test_cupy_optional()
    
    print("\n" + "=" * 50)
    print("üìä Summary:")
    print("=" * 50)
    
    if pytorch_ok and onnx_ok:
        print("‚úÖ GPU acceleration is FULLY SUPPORTED")
        print("   Your system is ready for GPU-accelerated emotion recognition!")
    elif onnx_ok:
        print("‚ö†Ô∏è Partial GPU support - ONNX Runtime GPU ready, PyTorch needs CUDA")
        print("   Face recognition will use GPU, but EEG processing will use CPU")
    elif pytorch_ok:
        print("‚ö†Ô∏è Partial GPU support - PyTorch CUDA ready, ONNX Runtime needs GPU")
        print("   EEG processing will use GPU, but face recognition will use CPU")
    else:
        print("‚ùå NO GPU support detected")
        print("   All processing will run on CPU")
    
    if cupy_ok:
        print("üöÄ CuPy available for maximum EEG processing performance")
    
    print("\nüìã Next Steps:")
    if not pytorch_ok:
        print("1. Run: install_gpu_packages.bat")
        print("   or manually: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
    
    if not onnx_ok:
        print("2. Install ONNX Runtime GPU: pip install onnxruntime-gpu")
    
    if not cupy_ok and (pytorch_ok or onnx_ok):
        print("3. Optional: Install CuPy for maximum performance: pip install cupy-cuda12x")
    
    print("\nüéØ Your emotion recognition system will automatically:")
    print("   - Use GPU when available")
    print("   - Fallback to CPU when GPU is not available")
    print("   - Show status messages during startup")

if __name__ == "__main__":
    main()
